{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2 (4)(b).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfBuSttHBCR-",
        "colab_type": "code",
        "outputId": "6091d5bc-76bb-421f-8eb1-c9fb54fd92f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0-rc3\n",
            "2.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DbpSoy-Brnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnDUprQUB4nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fb0xRQdCTgl",
        "colab_type": "code",
        "outputId": "8639c555-169d-4753-b2ee-18b11e308bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://833a97ef.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc5liJbVCzhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc \n",
        "try:\n",
        "    from StringIO import StringIO  # Python 2.7\n",
        "except ImportError:\n",
        "    from io import BytesIO         # Python 3.x\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
        "        self.writer = tf.summary.FileWriter(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Log a scalar variable.\"\"\"\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "\n",
        "    def image_summary(self, tag, images, step):\n",
        "        \"\"\"Log a list of images.\"\"\"\n",
        "\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            # Write the image to a string\n",
        "            try:\n",
        "                s = StringIO()\n",
        "            except:\n",
        "                s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
        "                                       height=img.shape[0],\n",
        "                                       width=img.shape[1])\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        \n",
        "    def histo_summary(self, tag, values, step, bins=1000):\n",
        "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
        "\n",
        "        # Create a histogram using numpy\n",
        "        counts, bin_edges = np.histogram(values, bins=bins)\n",
        "\n",
        "        # Fill the fields of the histogram proto\n",
        "        hist = tf.HistogramProto()\n",
        "        hist.min = float(np.min(values))\n",
        "        hist.max = float(np.max(values))\n",
        "        hist.num = int(np.prod(values.shape))\n",
        "        hist.sum = float(np.sum(values))\n",
        "        hist.sum_squares = float(np.sum(values**2))\n",
        "\n",
        "        # Drop the start of the first bin\n",
        "        bin_edges = bin_edges[1:]\n",
        "\n",
        "        # Add bin edges and counts\n",
        "        for edge in bin_edges:\n",
        "            hist.bucket_limit.append(edge)\n",
        "        for c in counts:\n",
        "            hist.bucket.append(c)\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzWEDYNdHVWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch and torchvision imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "logger = Logger('./logs')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmZAHM4aHXd0",
        "colab_type": "code",
        "outputId": "d534ac4d-8c9f-4387-b981-9844e70b357e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# Reading in the dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# Defining the model\n",
        "class View(nn.Module):\n",
        "    def __init__(self,o):\n",
        "        super().__init__()\n",
        "        self.o = o\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x.view(-1, self.o)\n",
        "    \n",
        "class allcnn_t(nn.Module):\n",
        "    def __init__(self, c1=96, c2= 192):\n",
        "        super().__init__()\n",
        "        d = 0.5\n",
        "\n",
        "        def convbn(ci,co,ksz,s=1,pz=0):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
        "                nn.ReLU(True),\n",
        "                nn.BatchNorm2d(co))\n",
        "\n",
        "        self.m = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            convbn(3,c1,3,1,1),\n",
        "            convbn(c1,c1,3,1,1),\n",
        "            convbn(c1,c1,3,2,1),\n",
        "            nn.Dropout(d),\n",
        "            convbn(c1,c2,3,1,1),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,c2,3,2,1),\n",
        "            nn.Dropout(d),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,c2,3,1,1),\n",
        "            convbn(c2,10,1,1),\n",
        "            nn.AvgPool2d(8),\n",
        "            View(10))\n",
        "\n",
        "        print('Num parameters: ', sum([p.numel() for p in self.m.parameters()]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.m(x)\n",
        "\n",
        "# The training loop\n",
        "train_loss_array = []\n",
        "lr_array = []\n",
        "def train(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot):\n",
        "    model = net.to(device)\n",
        "    total_step = len(train_loader)\n",
        "    overall_step = 0\n",
        "    update_lr = 0.00001\n",
        "    for epoch in range(epochs):\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=update_lr, weight_decay=0.001)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            for g in optimizer.param_groups:\n",
        "              g['lr'] = update_lr\n",
        "#             optimizer = optim.SGD(model.parameters(), lr=update_lr,momentum=0.9, weight_decay=0.001)\n",
        "            # Move tensors to configured device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #Forward Pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print (update_lr)\n",
        "#             if (i+1) % 2 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            train_loss_array.append(loss.item())\n",
        "            lr_array.append(update_lr)\n",
        "            if plot:\n",
        "              info = { ('loss_' + model_name): loss.item() }\n",
        "\n",
        "              for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, overall_step+1)\n",
        "            update_lr = 1.1*update_lr\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "model = allcnn_t().to(device)\n",
        "#TODO: Set it as number of epochs states in the question\n",
        "epochs = 4\n",
        "# TODO: Define the loss function as asked in the question\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# TODO: Set parameters as stated in the question\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.9,weight_decay=0.001)\n",
        "# Training loop called here\n",
        "train(model, optimizer, criterion, trainloader, testloader, epochs, 'cnn', True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num parameters:  1667166\n",
            "1e-05\n",
            "Epoch [1/4], Step [1/391], Loss: 2.3505\n",
            "1.0500000000000001e-05\n",
            "Epoch [1/4], Step [2/391], Loss: 2.3532\n",
            "1.1025000000000002e-05\n",
            "Epoch [1/4], Step [3/391], Loss: 2.3542\n",
            "1.1576250000000003e-05\n",
            "Epoch [1/4], Step [4/391], Loss: 2.3704\n",
            "1.2155062500000004e-05\n",
            "Epoch [1/4], Step [5/391], Loss: 2.3393\n",
            "1.2762815625000004e-05\n",
            "Epoch [1/4], Step [6/391], Loss: 2.3266\n",
            "1.3400956406250006e-05\n",
            "Epoch [1/4], Step [7/391], Loss: 2.3272\n",
            "1.4071004226562506e-05\n",
            "Epoch [1/4], Step [8/391], Loss: 2.3373\n",
            "1.4774554437890633e-05\n",
            "Epoch [1/4], Step [9/391], Loss: 2.3499\n",
            "1.5513282159785166e-05\n",
            "Epoch [1/4], Step [10/391], Loss: 2.3545\n",
            "1.6288946267774425e-05\n",
            "Epoch [1/4], Step [11/391], Loss: 2.3664\n",
            "1.7103393581163146e-05\n",
            "Epoch [1/4], Step [12/391], Loss: 2.3612\n",
            "1.7958563260221305e-05\n",
            "Epoch [1/4], Step [13/391], Loss: 2.3357\n",
            "1.8856491423232372e-05\n",
            "Epoch [1/4], Step [14/391], Loss: 2.3492\n",
            "1.9799315994393993e-05\n",
            "Epoch [1/4], Step [15/391], Loss: 2.3505\n",
            "2.0789281794113692e-05\n",
            "Epoch [1/4], Step [16/391], Loss: 2.3704\n",
            "2.1828745883819378e-05\n",
            "Epoch [1/4], Step [17/391], Loss: 2.3292\n",
            "2.292018317801035e-05\n",
            "Epoch [1/4], Step [18/391], Loss: 2.3449\n",
            "2.4066192336910866e-05\n",
            "Epoch [1/4], Step [19/391], Loss: 2.3407\n",
            "2.526950195375641e-05\n",
            "Epoch [1/4], Step [20/391], Loss: 2.3578\n",
            "2.6532977051444233e-05\n",
            "Epoch [1/4], Step [21/391], Loss: 2.3351\n",
            "2.7859625904016447e-05\n",
            "Epoch [1/4], Step [22/391], Loss: 2.3398\n",
            "2.925260719921727e-05\n",
            "Epoch [1/4], Step [23/391], Loss: 2.3437\n",
            "3.071523755917814e-05\n",
            "Epoch [1/4], Step [24/391], Loss: 2.3657\n",
            "3.2250999437137045e-05\n",
            "Epoch [1/4], Step [25/391], Loss: 2.3450\n",
            "3.3863549408993897e-05\n",
            "Epoch [1/4], Step [26/391], Loss: 2.3510\n",
            "3.5556726879443595e-05\n",
            "Epoch [1/4], Step [27/391], Loss: 2.3212\n",
            "3.7334563223415774e-05\n",
            "Epoch [1/4], Step [28/391], Loss: 2.3256\n",
            "3.9201291384586564e-05\n",
            "Epoch [1/4], Step [29/391], Loss: 2.3457\n",
            "4.1161355953815896e-05\n",
            "Epoch [1/4], Step [30/391], Loss: 2.3374\n",
            "4.321942375150669e-05\n",
            "Epoch [1/4], Step [31/391], Loss: 2.3240\n",
            "4.5380394939082024e-05\n",
            "Epoch [1/4], Step [32/391], Loss: 2.3339\n",
            "4.7649414686036124e-05\n",
            "Epoch [1/4], Step [33/391], Loss: 2.3269\n",
            "5.0031885420337935e-05\n",
            "Epoch [1/4], Step [34/391], Loss: 2.3261\n",
            "5.2533479691354834e-05\n",
            "Epoch [1/4], Step [35/391], Loss: 2.3599\n",
            "5.516015367592258e-05\n",
            "Epoch [1/4], Step [36/391], Loss: 2.3300\n",
            "5.7918161359718715e-05\n",
            "Epoch [1/4], Step [37/391], Loss: 2.3343\n",
            "6.081406942770465e-05\n",
            "Epoch [1/4], Step [38/391], Loss: 2.3383\n",
            "6.385477289908989e-05\n",
            "Epoch [1/4], Step [39/391], Loss: 2.3314\n",
            "6.704751154404438e-05\n",
            "Epoch [1/4], Step [40/391], Loss: 2.3097\n",
            "7.039988712124661e-05\n",
            "Epoch [1/4], Step [41/391], Loss: 2.3188\n",
            "7.391988147730895e-05\n",
            "Epoch [1/4], Step [42/391], Loss: 2.3339\n",
            "7.761587555117439e-05\n",
            "Epoch [1/4], Step [43/391], Loss: 2.2992\n",
            "8.149666932873311e-05\n",
            "Epoch [1/4], Step [44/391], Loss: 2.3202\n",
            "8.557150279516977e-05\n",
            "Epoch [1/4], Step [45/391], Loss: 2.3307\n",
            "8.985007793492826e-05\n",
            "Epoch [1/4], Step [46/391], Loss: 2.2969\n",
            "9.434258183167468e-05\n",
            "Epoch [1/4], Step [47/391], Loss: 2.3220\n",
            "9.905971092325841e-05\n",
            "Epoch [1/4], Step [48/391], Loss: 2.3188\n",
            "0.00010401269646942134\n",
            "Epoch [1/4], Step [49/391], Loss: 2.2992\n",
            "0.00010921333129289241\n",
            "Epoch [1/4], Step [50/391], Loss: 2.3350\n",
            "0.00011467399785753704\n",
            "Epoch [1/4], Step [51/391], Loss: 2.3271\n",
            "0.0001204076977504139\n",
            "Epoch [1/4], Step [52/391], Loss: 2.3226\n",
            "0.0001264280826379346\n",
            "Epoch [1/4], Step [53/391], Loss: 2.3301\n",
            "0.00013274948676983132\n",
            "Epoch [1/4], Step [54/391], Loss: 2.2902\n",
            "0.0001393869611083229\n",
            "Epoch [1/4], Step [55/391], Loss: 2.3068\n",
            "0.00014635630916373904\n",
            "Epoch [1/4], Step [56/391], Loss: 2.3137\n",
            "0.000153674124621926\n",
            "Epoch [1/4], Step [57/391], Loss: 2.3116\n",
            "0.0001613578308530223\n",
            "Epoch [1/4], Step [58/391], Loss: 2.2923\n",
            "0.00016942572239567343\n",
            "Epoch [1/4], Step [59/391], Loss: 2.3035\n",
            "0.0001778970085154571\n",
            "Epoch [1/4], Step [60/391], Loss: 2.2826\n",
            "0.00018679185894122997\n",
            "Epoch [1/4], Step [61/391], Loss: 2.2855\n",
            "0.00019613145188829149\n",
            "Epoch [1/4], Step [62/391], Loss: 2.2800\n",
            "0.00020593802448270606\n",
            "Epoch [1/4], Step [63/391], Loss: 2.2906\n",
            "0.00021623492570684136\n",
            "Epoch [1/4], Step [64/391], Loss: 2.2749\n",
            "0.00022704667199218342\n",
            "Epoch [1/4], Step [65/391], Loss: 2.2784\n",
            "0.0002383990055917926\n",
            "Epoch [1/4], Step [66/391], Loss: 2.2802\n",
            "0.00025031895587138223\n",
            "Epoch [1/4], Step [67/391], Loss: 2.3035\n",
            "0.00026283490366495134\n",
            "Epoch [1/4], Step [68/391], Loss: 2.2699\n",
            "0.0002759766488481989\n",
            "Epoch [1/4], Step [69/391], Loss: 2.2783\n",
            "0.00028977548129060886\n",
            "Epoch [1/4], Step [70/391], Loss: 2.2817\n",
            "0.0003042642553551393\n",
            "Epoch [1/4], Step [71/391], Loss: 2.2472\n",
            "0.0003194774681228963\n",
            "Epoch [1/4], Step [72/391], Loss: 2.2392\n",
            "0.00033545134152904117\n",
            "Epoch [1/4], Step [73/391], Loss: 2.2676\n",
            "0.00035222390860549323\n",
            "Epoch [1/4], Step [74/391], Loss: 2.2576\n",
            "0.0003698351040357679\n",
            "Epoch [1/4], Step [75/391], Loss: 2.2596\n",
            "0.0003883268592375563\n",
            "Epoch [1/4], Step [76/391], Loss: 2.2638\n",
            "0.00040774320219943413\n",
            "Epoch [1/4], Step [77/391], Loss: 2.2371\n",
            "0.0004281303623094059\n",
            "Epoch [1/4], Step [78/391], Loss: 2.2452\n",
            "0.0004495368804248762\n",
            "Epoch [1/4], Step [79/391], Loss: 2.2249\n",
            "0.00047201372444612003\n",
            "Epoch [1/4], Step [80/391], Loss: 2.2593\n",
            "0.000495614410668426\n",
            "Epoch [1/4], Step [81/391], Loss: 2.2199\n",
            "0.0005203951312018473\n",
            "Epoch [1/4], Step [82/391], Loss: 2.2157\n",
            "0.0005464148877619398\n",
            "Epoch [1/4], Step [83/391], Loss: 2.2317\n",
            "0.0005737356321500368\n",
            "Epoch [1/4], Step [84/391], Loss: 2.2380\n",
            "0.0006024224137575387\n",
            "Epoch [1/4], Step [85/391], Loss: 2.2160\n",
            "0.0006325435344454156\n",
            "Epoch [1/4], Step [86/391], Loss: 2.2462\n",
            "0.0006641707111676864\n",
            "Epoch [1/4], Step [87/391], Loss: 2.1895\n",
            "0.0006973792467260708\n",
            "Epoch [1/4], Step [88/391], Loss: 2.1867\n",
            "0.0007322482090623744\n",
            "Epoch [1/4], Step [89/391], Loss: 2.2193\n",
            "0.0007688606195154932\n",
            "Epoch [1/4], Step [90/391], Loss: 2.1935\n",
            "0.0008073036504912678\n",
            "Epoch [1/4], Step [91/391], Loss: 2.1446\n",
            "0.0008476688330158312\n",
            "Epoch [1/4], Step [92/391], Loss: 2.1840\n",
            "0.0008900522746666228\n",
            "Epoch [1/4], Step [93/391], Loss: 2.2110\n",
            "0.000934554888399954\n",
            "Epoch [1/4], Step [94/391], Loss: 2.1657\n",
            "0.0009812826328199516\n",
            "Epoch [1/4], Step [95/391], Loss: 2.1423\n",
            "0.0010303467644609493\n",
            "Epoch [1/4], Step [96/391], Loss: 2.1425\n",
            "0.0010818641026839968\n",
            "Epoch [1/4], Step [97/391], Loss: 2.1717\n",
            "0.0011359573078181967\n",
            "Epoch [1/4], Step [98/391], Loss: 2.1173\n",
            "0.0011927551732091065\n",
            "Epoch [1/4], Step [99/391], Loss: 2.1366\n",
            "0.001252392931869562\n",
            "Epoch [1/4], Step [100/391], Loss: 2.1241\n",
            "0.00131501257846304\n",
            "Epoch [1/4], Step [101/391], Loss: 2.1379\n",
            "0.0013807632073861921\n",
            "Epoch [1/4], Step [102/391], Loss: 2.1085\n",
            "0.0014498013677555017\n",
            "Epoch [1/4], Step [103/391], Loss: 2.0243\n",
            "0.001522291436143277\n",
            "Epoch [1/4], Step [104/391], Loss: 2.0693\n",
            "0.0015984060079504408\n",
            "Epoch [1/4], Step [105/391], Loss: 2.0528\n",
            "0.001678326308347963\n",
            "Epoch [1/4], Step [106/391], Loss: 2.0569\n",
            "0.0017622426237653612\n",
            "Epoch [1/4], Step [107/391], Loss: 2.0420\n",
            "0.0018503547549536294\n",
            "Epoch [1/4], Step [108/391], Loss: 2.0932\n",
            "0.001942872492701311\n",
            "Epoch [1/4], Step [109/391], Loss: 2.0902\n",
            "0.0020400161173363767\n",
            "Epoch [1/4], Step [110/391], Loss: 1.9821\n",
            "0.002142016923203196\n",
            "Epoch [1/4], Step [111/391], Loss: 2.0177\n",
            "0.0022491177693633557\n",
            "Epoch [1/4], Step [112/391], Loss: 2.0516\n",
            "0.0023615736578315237\n",
            "Epoch [1/4], Step [113/391], Loss: 1.9776\n",
            "0.0024796523407231\n",
            "Epoch [1/4], Step [114/391], Loss: 2.0723\n",
            "0.0026036349577592552\n",
            "Epoch [1/4], Step [115/391], Loss: 2.0546\n",
            "0.002733816705647218\n",
            "Epoch [1/4], Step [116/391], Loss: 2.0058\n",
            "0.0028705075409295793\n",
            "Epoch [1/4], Step [117/391], Loss: 1.9804\n",
            "0.0030140329179760583\n",
            "Epoch [1/4], Step [118/391], Loss: 1.9857\n",
            "0.0031647345638748615\n",
            "Epoch [1/4], Step [119/391], Loss: 2.0142\n",
            "0.0033229712920686046\n",
            "Epoch [1/4], Step [120/391], Loss: 1.9599\n",
            "0.003489119856672035\n",
            "Epoch [1/4], Step [121/391], Loss: 2.0634\n",
            "0.003663575849505637\n",
            "Epoch [1/4], Step [122/391], Loss: 1.9827\n",
            "0.003846754641980919\n",
            "Epoch [1/4], Step [123/391], Loss: 1.9411\n",
            "0.004039092374079965\n",
            "Epoch [1/4], Step [124/391], Loss: 1.9371\n",
            "0.004241046992783964\n",
            "Epoch [1/4], Step [125/391], Loss: 2.0032\n",
            "0.004453099342423162\n",
            "Epoch [1/4], Step [126/391], Loss: 1.9486\n",
            "0.004675754309544321\n",
            "Epoch [1/4], Step [127/391], Loss: 1.9742\n",
            "0.004909542025021537\n",
            "Epoch [1/4], Step [128/391], Loss: 2.0154\n",
            "0.005155019126272614\n",
            "Epoch [1/4], Step [129/391], Loss: 1.9335\n",
            "0.005412770082586245\n",
            "Epoch [1/4], Step [130/391], Loss: 1.9513\n",
            "0.005683408586715557\n",
            "Epoch [1/4], Step [131/391], Loss: 1.8243\n",
            "0.005967579016051335\n",
            "Epoch [1/4], Step [132/391], Loss: 1.9169\n",
            "0.0062659579668539024\n",
            "Epoch [1/4], Step [133/391], Loss: 1.8871\n",
            "0.0065792558651965975\n",
            "Epoch [1/4], Step [134/391], Loss: 1.9438\n",
            "0.006908218658456428\n",
            "Epoch [1/4], Step [135/391], Loss: 1.9319\n",
            "0.0072536295913792495\n",
            "Epoch [1/4], Step [136/391], Loss: 1.8313\n",
            "0.007616311070948212\n",
            "Epoch [1/4], Step [137/391], Loss: 1.9075\n",
            "0.007997126624495624\n",
            "Epoch [1/4], Step [138/391], Loss: 1.9683\n",
            "0.008396982955720405\n",
            "Epoch [1/4], Step [139/391], Loss: 1.9047\n",
            "0.008816832103506426\n",
            "Epoch [1/4], Step [140/391], Loss: 1.8808\n",
            "0.009257673708681748\n",
            "Epoch [1/4], Step [141/391], Loss: 1.9725\n",
            "0.009720557394115835\n",
            "Epoch [1/4], Step [142/391], Loss: 1.8952\n",
            "0.010206585263821627\n",
            "Epoch [1/4], Step [143/391], Loss: 1.9326\n",
            "0.010716914527012709\n",
            "Epoch [1/4], Step [144/391], Loss: 1.7760\n",
            "0.011252760253363345\n",
            "Epoch [1/4], Step [145/391], Loss: 1.8568\n",
            "0.011815398266031512\n",
            "Epoch [1/4], Step [146/391], Loss: 1.8544\n",
            "0.012406168179333089\n",
            "Epoch [1/4], Step [147/391], Loss: 1.7889\n",
            "0.013026476588299744\n",
            "Epoch [1/4], Step [148/391], Loss: 1.8752\n",
            "0.013677800417714733\n",
            "Epoch [1/4], Step [149/391], Loss: 1.8570\n",
            "0.01436169043860047\n",
            "Epoch [1/4], Step [150/391], Loss: 1.8205\n",
            "0.015079774960530494\n",
            "Epoch [1/4], Step [151/391], Loss: 1.8313\n",
            "0.01583376370855702\n",
            "Epoch [1/4], Step [152/391], Loss: 1.9177\n",
            "0.01662545189398487\n",
            "Epoch [1/4], Step [153/391], Loss: 1.8589\n",
            "0.017456724488684117\n",
            "Epoch [1/4], Step [154/391], Loss: 1.8547\n",
            "0.018329560713118323\n",
            "Epoch [1/4], Step [155/391], Loss: 1.8622\n",
            "0.01924603874877424\n",
            "Epoch [1/4], Step [156/391], Loss: 1.8610\n",
            "0.020208340686212953\n",
            "Epoch [1/4], Step [157/391], Loss: 1.8837\n",
            "0.0212187577205236\n",
            "Epoch [1/4], Step [158/391], Loss: 1.8217\n",
            "0.02227969560654978\n",
            "Epoch [1/4], Step [159/391], Loss: 1.8589\n",
            "0.02339368038687727\n",
            "Epoch [1/4], Step [160/391], Loss: 1.9706\n",
            "0.024563364406221134\n",
            "Epoch [1/4], Step [161/391], Loss: 1.8474\n",
            "0.025791532626532193\n",
            "Epoch [1/4], Step [162/391], Loss: 1.8879\n",
            "0.027081109257858803\n",
            "Epoch [1/4], Step [163/391], Loss: 1.8826\n",
            "0.028435164720751745\n",
            "Epoch [1/4], Step [164/391], Loss: 1.8298\n",
            "0.029856922956789333\n",
            "Epoch [1/4], Step [165/391], Loss: 1.7873\n",
            "0.0313497691046288\n",
            "Epoch [1/4], Step [166/391], Loss: 1.8567\n",
            "0.03291725755986024\n",
            "Epoch [1/4], Step [167/391], Loss: 1.8978\n",
            "0.034563120437853256\n",
            "Epoch [1/4], Step [168/391], Loss: 1.7894\n",
            "0.03629127645974592\n",
            "Epoch [1/4], Step [169/391], Loss: 1.8409\n",
            "0.038105840282733214\n",
            "Epoch [1/4], Step [170/391], Loss: 1.9184\n",
            "0.040011132296869874\n",
            "Epoch [1/4], Step [171/391], Loss: 1.9310\n",
            "0.04201168891171337\n",
            "Epoch [1/4], Step [172/391], Loss: 1.8205\n",
            "0.04411227335729904\n",
            "Epoch [1/4], Step [173/391], Loss: 1.7082\n",
            "0.04631788702516399\n",
            "Epoch [1/4], Step [174/391], Loss: 1.7389\n",
            "0.04863378137642219\n",
            "Epoch [1/4], Step [175/391], Loss: 1.8693\n",
            "0.0510654704452433\n",
            "Epoch [1/4], Step [176/391], Loss: 1.9519\n",
            "0.05361874396750547\n",
            "Epoch [1/4], Step [177/391], Loss: 1.7664\n",
            "0.056299681165880744\n",
            "Epoch [1/4], Step [178/391], Loss: 1.8238\n",
            "0.05911466522417479\n",
            "Epoch [1/4], Step [179/391], Loss: 1.8215\n",
            "0.06207039848538353\n",
            "Epoch [1/4], Step [180/391], Loss: 1.8242\n",
            "0.06517391840965271\n",
            "Epoch [1/4], Step [181/391], Loss: 1.6874\n",
            "0.06843261433013535\n",
            "Epoch [1/4], Step [182/391], Loss: 1.7672\n",
            "0.07185424504664212\n",
            "Epoch [1/4], Step [183/391], Loss: 1.7427\n",
            "0.07544695729897423\n",
            "Epoch [1/4], Step [184/391], Loss: 1.7340\n",
            "0.07921930516392295\n",
            "Epoch [1/4], Step [185/391], Loss: 1.8736\n",
            "0.0831802704221191\n",
            "Epoch [1/4], Step [186/391], Loss: 1.8545\n",
            "0.08733928394322506\n",
            "Epoch [1/4], Step [187/391], Loss: 1.8565\n",
            "0.09170624814038632\n",
            "Epoch [1/4], Step [188/391], Loss: 1.7977\n",
            "0.09629156054740563\n",
            "Epoch [1/4], Step [189/391], Loss: 1.8741\n",
            "0.10110613857477592\n",
            "Epoch [1/4], Step [190/391], Loss: 1.7725\n",
            "0.10616144550351472\n",
            "Epoch [1/4], Step [191/391], Loss: 1.9666\n",
            "0.11146951777869046\n",
            "Epoch [1/4], Step [192/391], Loss: 1.7495\n",
            "0.117042993667625\n",
            "Epoch [1/4], Step [193/391], Loss: 1.7897\n",
            "0.12289514335100625\n",
            "Epoch [1/4], Step [194/391], Loss: 1.8290\n",
            "0.12903990051855657\n",
            "Epoch [1/4], Step [195/391], Loss: 1.7118\n",
            "0.1354918955444844\n",
            "Epoch [1/4], Step [196/391], Loss: 1.7822\n",
            "0.14226649032170863\n",
            "Epoch [1/4], Step [197/391], Loss: 1.7595\n",
            "0.14937981483779406\n",
            "Epoch [1/4], Step [198/391], Loss: 1.7631\n",
            "0.15684880557968375\n",
            "Epoch [1/4], Step [199/391], Loss: 1.7192\n",
            "0.16469124585866796\n",
            "Epoch [1/4], Step [200/391], Loss: 1.6578\n",
            "0.17292580815160136\n",
            "Epoch [1/4], Step [201/391], Loss: 1.6896\n",
            "0.18157209855918144\n",
            "Epoch [1/4], Step [202/391], Loss: 1.7616\n",
            "0.19065070348714053\n",
            "Epoch [1/4], Step [203/391], Loss: 1.6510\n",
            "0.20018323866149756\n",
            "Epoch [1/4], Step [204/391], Loss: 1.7197\n",
            "0.21019240059457245\n",
            "Epoch [1/4], Step [205/391], Loss: 1.6052\n",
            "0.22070202062430108\n",
            "Epoch [1/4], Step [206/391], Loss: 1.8257\n",
            "0.23173712165551616\n",
            "Epoch [1/4], Step [207/391], Loss: 1.6284\n",
            "0.24332397773829198\n",
            "Epoch [1/4], Step [208/391], Loss: 1.7437\n",
            "0.25549017662520657\n",
            "Epoch [1/4], Step [209/391], Loss: 1.7692\n",
            "0.2682646854564669\n",
            "Epoch [1/4], Step [210/391], Loss: 1.7290\n",
            "0.2816779197292903\n",
            "Epoch [1/4], Step [211/391], Loss: 1.6543\n",
            "0.29576181571575483\n",
            "Epoch [1/4], Step [212/391], Loss: 1.8557\n",
            "0.31054990650154257\n",
            "Epoch [1/4], Step [213/391], Loss: 1.8818\n",
            "0.3260774018266197\n",
            "Epoch [1/4], Step [214/391], Loss: 1.7519\n",
            "0.3423812719179507\n",
            "Epoch [1/4], Step [215/391], Loss: 1.7223\n",
            "0.35950033551384825\n",
            "Epoch [1/4], Step [216/391], Loss: 1.8635\n",
            "0.37747535228954066\n",
            "Epoch [1/4], Step [217/391], Loss: 1.8293\n",
            "0.3963491199040177\n",
            "Epoch [1/4], Step [218/391], Loss: 1.8060\n",
            "0.4161665758992186\n",
            "Epoch [1/4], Step [219/391], Loss: 1.7079\n",
            "0.4369749046941796\n",
            "Epoch [1/4], Step [220/391], Loss: 1.6296\n",
            "0.4588236499288886\n",
            "Epoch [1/4], Step [221/391], Loss: 1.8387\n",
            "0.481764832425333\n",
            "Epoch [1/4], Step [222/391], Loss: 1.8217\n",
            "0.5058530740465997\n",
            "Epoch [1/4], Step [223/391], Loss: 1.7556\n",
            "0.5311457277489298\n",
            "Epoch [1/4], Step [224/391], Loss: 1.8357\n",
            "0.5577030141363762\n",
            "Epoch [1/4], Step [225/391], Loss: 1.9002\n",
            "0.5855881648431951\n",
            "Epoch [1/4], Step [226/391], Loss: 1.8874\n",
            "0.6148675730853549\n",
            "Epoch [1/4], Step [227/391], Loss: 1.8028\n",
            "0.6456109517396227\n",
            "Epoch [1/4], Step [228/391], Loss: 1.7840\n",
            "0.6778914993266039\n",
            "Epoch [1/4], Step [229/391], Loss: 1.7100\n",
            "0.7117860742929341\n",
            "Epoch [1/4], Step [230/391], Loss: 1.8357\n",
            "0.7473753780075808\n",
            "Epoch [1/4], Step [231/391], Loss: 1.8962\n",
            "0.7847441469079599\n",
            "Epoch [1/4], Step [232/391], Loss: 1.7548\n",
            "0.823981354253358\n",
            "Epoch [1/4], Step [233/391], Loss: 1.8212\n",
            "0.8651804219660258\n",
            "Epoch [1/4], Step [234/391], Loss: 1.7959\n",
            "0.9084394430643272\n",
            "Epoch [1/4], Step [235/391], Loss: 1.6921\n",
            "0.9538614152175436\n",
            "Epoch [1/4], Step [236/391], Loss: 1.6889\n",
            "1.001554485978421\n",
            "Epoch [1/4], Step [237/391], Loss: 1.7354\n",
            "1.0516322102773419\n",
            "Epoch [1/4], Step [238/391], Loss: 1.8916\n",
            "1.104213820791209\n",
            "Epoch [1/4], Step [239/391], Loss: 1.8343\n",
            "1.1594245118307696\n",
            "Epoch [1/4], Step [240/391], Loss: 1.5248\n",
            "1.217395737422308\n",
            "Epoch [1/4], Step [241/391], Loss: 1.5812\n",
            "1.2782655242934235\n",
            "Epoch [1/4], Step [242/391], Loss: 1.8851\n",
            "1.3421788005080948\n",
            "Epoch [1/4], Step [243/391], Loss: 1.7692\n",
            "1.4092877405334996\n",
            "Epoch [1/4], Step [244/391], Loss: 1.6445\n",
            "1.4797521275601746\n",
            "Epoch [1/4], Step [245/391], Loss: 1.7244\n",
            "1.5537397339381833\n",
            "Epoch [1/4], Step [246/391], Loss: 1.7327\n",
            "1.6314267206350925\n",
            "Epoch [1/4], Step [247/391], Loss: 1.8838\n",
            "1.7129980566668472\n",
            "Epoch [1/4], Step [248/391], Loss: 1.8206\n",
            "1.7986479595001896\n",
            "Epoch [1/4], Step [249/391], Loss: 1.7707\n",
            "1.8885803574751991\n",
            "Epoch [1/4], Step [250/391], Loss: 1.9774\n",
            "1.9830093753489593\n",
            "Epoch [1/4], Step [251/391], Loss: 1.6572\n",
            "2.0821598441164073\n",
            "Epoch [1/4], Step [252/391], Loss: 1.8954\n",
            "2.186267836322228\n",
            "Epoch [1/4], Step [253/391], Loss: 1.9936\n",
            "2.295581228138339\n",
            "Epoch [1/4], Step [254/391], Loss: 1.9090\n",
            "2.410360289545256\n",
            "Epoch [1/4], Step [255/391], Loss: 1.7659\n",
            "2.530878304022519\n",
            "Epoch [1/4], Step [256/391], Loss: 2.0086\n",
            "2.6574222192236454\n",
            "Epoch [1/4], Step [257/391], Loss: 1.7799\n",
            "2.7902933301848276\n",
            "Epoch [1/4], Step [258/391], Loss: 1.9396\n",
            "2.9298079966940693\n",
            "Epoch [1/4], Step [259/391], Loss: 1.9673\n",
            "3.076298396528773\n",
            "Epoch [1/4], Step [260/391], Loss: 1.8583\n",
            "3.2301133163552116\n",
            "Epoch [1/4], Step [261/391], Loss: 1.9564\n",
            "3.3916189821729725\n",
            "Epoch [1/4], Step [262/391], Loss: 1.9106\n",
            "3.5611999312816214\n",
            "Epoch [1/4], Step [263/391], Loss: 2.0281\n",
            "3.7392599278457026\n",
            "Epoch [1/4], Step [264/391], Loss: 1.9474\n",
            "3.926222924237988\n",
            "Epoch [1/4], Step [265/391], Loss: 1.8601\n",
            "4.122534070449888\n",
            "Epoch [1/4], Step [266/391], Loss: 2.0635\n",
            "4.328660773972382\n",
            "Epoch [1/4], Step [267/391], Loss: 2.0843\n",
            "4.545093812671002\n",
            "Epoch [1/4], Step [268/391], Loss: 2.2358\n",
            "4.772348503304553\n",
            "Epoch [1/4], Step [269/391], Loss: 1.9621\n",
            "5.010965928469781\n",
            "Epoch [1/4], Step [270/391], Loss: 2.1206\n",
            "5.26151422489327\n",
            "Epoch [1/4], Step [271/391], Loss: 2.3083\n",
            "5.524589936137934\n",
            "Epoch [1/4], Step [272/391], Loss: 2.1181\n",
            "5.800819432944831\n",
            "Epoch [1/4], Step [273/391], Loss: 2.1609\n",
            "6.090860404592073\n",
            "Epoch [1/4], Step [274/391], Loss: 2.2704\n",
            "6.3954034248216765\n",
            "Epoch [1/4], Step [275/391], Loss: 2.2884\n",
            "6.715173596062761\n",
            "Epoch [1/4], Step [276/391], Loss: 2.1962\n",
            "7.050932275865899\n",
            "Epoch [1/4], Step [277/391], Loss: 2.1574\n",
            "7.403478889659195\n",
            "Epoch [1/4], Step [278/391], Loss: 2.2725\n",
            "7.773652834142155\n",
            "Epoch [1/4], Step [279/391], Loss: 2.1597\n",
            "8.162335475849263\n",
            "Epoch [1/4], Step [280/391], Loss: 2.1604\n",
            "8.570452249641725\n",
            "Epoch [1/4], Step [281/391], Loss: 2.3764\n",
            "8.998974862123813\n",
            "Epoch [1/4], Step [282/391], Loss: 2.3648\n",
            "9.448923605230004\n",
            "Epoch [1/4], Step [283/391], Loss: 2.5528\n",
            "9.921369785491505\n",
            "Epoch [1/4], Step [284/391], Loss: 2.2546\n",
            "10.41743827476608\n",
            "Epoch [1/4], Step [285/391], Loss: 2.4616\n",
            "10.938310188504385\n",
            "Epoch [1/4], Step [286/391], Loss: 2.4610\n",
            "11.485225697929605\n",
            "Epoch [1/4], Step [287/391], Loss: 2.4695\n",
            "12.059486982826087\n",
            "Epoch [1/4], Step [288/391], Loss: 2.5314\n",
            "12.662461331967393\n",
            "Epoch [1/4], Step [289/391], Loss: 2.5687\n",
            "13.295584398565763\n",
            "Epoch [1/4], Step [290/391], Loss: 2.5775\n",
            "13.960363618494052\n",
            "Epoch [1/4], Step [291/391], Loss: 2.5358\n",
            "14.658381799418756\n",
            "Epoch [1/4], Step [292/391], Loss: 2.9437\n",
            "15.391300889389694\n",
            "Epoch [1/4], Step [293/391], Loss: 3.0945\n",
            "16.160865933859178\n",
            "Epoch [1/4], Step [294/391], Loss: 2.4767\n",
            "16.96890923055214\n",
            "Epoch [1/4], Step [295/391], Loss: 3.9478\n",
            "17.817354692079746\n",
            "Epoch [1/4], Step [296/391], Loss: 3.8151\n",
            "18.708222426683733\n",
            "Epoch [1/4], Step [297/391], Loss: 3.5954\n",
            "19.64363354801792\n",
            "Epoch [1/4], Step [298/391], Loss: 5.7882\n",
            "20.625815225418815\n",
            "Epoch [1/4], Step [299/391], Loss: 5.4783\n",
            "21.657105986689757\n",
            "Epoch [1/4], Step [300/391], Loss: 5.3170\n",
            "22.739961286024247\n",
            "Epoch [1/4], Step [301/391], Loss: 6.4517\n",
            "23.87695935032546\n",
            "Epoch [1/4], Step [302/391], Loss: 8.1980\n",
            "25.070807317841734\n",
            "Epoch [1/4], Step [303/391], Loss: 10.5367\n",
            "26.324347683733823\n",
            "Epoch [1/4], Step [304/391], Loss: 10.6395\n",
            "27.640565067920516\n",
            "Epoch [1/4], Step [305/391], Loss: 13.9562\n",
            "29.022593321316542\n",
            "Epoch [1/4], Step [306/391], Loss: 16.5532\n",
            "30.47372298738237\n",
            "Epoch [1/4], Step [307/391], Loss: 17.6079\n",
            "31.99740913675149\n",
            "Epoch [1/4], Step [308/391], Loss: 19.0030\n",
            "33.597279593589064\n",
            "Epoch [1/4], Step [309/391], Loss: 22.3634\n",
            "35.27714357326852\n",
            "Epoch [1/4], Step [310/391], Loss: 23.6514\n",
            "37.041000751931946\n",
            "Epoch [1/4], Step [311/391], Loss: 23.4141\n",
            "38.893050789528544\n",
            "Epoch [1/4], Step [312/391], Loss: 27.0550\n",
            "40.837703329004974\n",
            "Epoch [1/4], Step [313/391], Loss: 40.6810\n",
            "42.87958849545522\n",
            "Epoch [1/4], Step [314/391], Loss: 18.1392\n",
            "45.02356792022799\n",
            "Epoch [1/4], Step [315/391], Loss: 31.6470\n",
            "47.274746316239394\n",
            "Epoch [1/4], Step [316/391], Loss: 41.9810\n",
            "49.63848363205137\n",
            "Epoch [1/4], Step [317/391], Loss: 44.1336\n",
            "52.12040781365394\n",
            "Epoch [1/4], Step [318/391], Loss: 40.4529\n",
            "54.72642820433664\n",
            "Epoch [1/4], Step [319/391], Loss: 37.3016\n",
            "57.46274961455347\n",
            "Epoch [1/4], Step [320/391], Loss: 54.7714\n",
            "60.33588709528115\n",
            "Epoch [1/4], Step [321/391], Loss: 58.0032\n",
            "63.35268145004521\n",
            "Epoch [1/4], Step [322/391], Loss: 51.3347\n",
            "66.52031552254748\n",
            "Epoch [1/4], Step [323/391], Loss: 55.0208\n",
            "69.84633129867485\n",
            "Epoch [1/4], Step [324/391], Loss: 56.1273\n",
            "73.3386478636086\n",
            "Epoch [1/4], Step [325/391], Loss: 47.8319\n",
            "77.00558025678903\n",
            "Epoch [1/4], Step [326/391], Loss: 51.2972\n",
            "80.85585926962848\n",
            "Epoch [1/4], Step [327/391], Loss: 62.9061\n",
            "84.89865223310991\n",
            "Epoch [1/4], Step [328/391], Loss: 57.7240\n",
            "89.14358484476541\n",
            "Epoch [1/4], Step [329/391], Loss: 84.8318\n",
            "93.60076408700368\n",
            "Epoch [1/4], Step [330/391], Loss: 85.1173\n",
            "98.28080229135387\n",
            "Epoch [1/4], Step [331/391], Loss: 89.9289\n",
            "103.19484240592156\n",
            "Epoch [1/4], Step [332/391], Loss: 108.1534\n",
            "108.35458452621765\n",
            "Epoch [1/4], Step [333/391], Loss: 102.5106\n",
            "113.77231375252853\n",
            "Epoch [1/4], Step [334/391], Loss: 129.0459\n",
            "119.46092944015497\n",
            "Epoch [1/4], Step [335/391], Loss: 84.0246\n",
            "125.43397591216272\n",
            "Epoch [1/4], Step [336/391], Loss: 125.1268\n",
            "131.70567470777087\n",
            "Epoch [1/4], Step [337/391], Loss: 122.2383\n",
            "138.2909584431594\n",
            "Epoch [1/4], Step [338/391], Loss: 141.5459\n",
            "145.20550636531738\n",
            "Epoch [1/4], Step [339/391], Loss: 173.7686\n",
            "152.46578168358326\n",
            "Epoch [1/4], Step [340/391], Loss: 155.8467\n",
            "160.08907076776242\n",
            "Epoch [1/4], Step [341/391], Loss: 131.3217\n",
            "168.09352430615056\n",
            "Epoch [1/4], Step [342/391], Loss: 139.9480\n",
            "176.4982005214581\n",
            "Epoch [1/4], Step [343/391], Loss: 127.5092\n",
            "185.323110547531\n",
            "Epoch [1/4], Step [344/391], Loss: 159.1781\n",
            "194.58926607490756\n",
            "Epoch [1/4], Step [345/391], Loss: 197.2988\n",
            "204.31872937865293\n",
            "Epoch [1/4], Step [346/391], Loss: 212.6669\n",
            "214.5346658475856\n",
            "Epoch [1/4], Step [347/391], Loss: 208.9954\n",
            "225.26139913996488\n",
            "Epoch [1/4], Step [348/391], Loss: 213.8673\n",
            "236.52446909696314\n",
            "Epoch [1/4], Step [349/391], Loss: 257.7364\n",
            "248.3506925518113\n",
            "Epoch [1/4], Step [350/391], Loss: 248.0834\n",
            "260.7682271794019\n",
            "Epoch [1/4], Step [351/391], Loss: 276.9484\n",
            "273.806638538372\n",
            "Epoch [1/4], Step [352/391], Loss: 225.2003\n",
            "287.4969704652906\n",
            "Epoch [1/4], Step [353/391], Loss: 261.5691\n",
            "301.8718189885551\n",
            "Epoch [1/4], Step [354/391], Loss: 298.3613\n",
            "316.9654099379829\n",
            "Epoch [1/4], Step [355/391], Loss: 294.4884\n",
            "332.81368043488203\n",
            "Epoch [1/4], Step [356/391], Loss: 315.8411\n",
            "349.45436445662614\n",
            "Epoch [1/4], Step [357/391], Loss: 331.1230\n",
            "366.92708267945744\n",
            "Epoch [1/4], Step [358/391], Loss: 370.6346\n",
            "385.27343681343035\n",
            "Epoch [1/4], Step [359/391], Loss: 305.5643\n",
            "404.53710865410187\n",
            "Epoch [1/4], Step [360/391], Loss: 270.8057\n",
            "424.763964086807\n",
            "Epoch [1/4], Step [361/391], Loss: 319.9013\n",
            "446.0021622911474\n",
            "Epoch [1/4], Step [362/391], Loss: 342.6934\n",
            "468.30227040570475\n",
            "Epoch [1/4], Step [363/391], Loss: 380.4343\n",
            "491.71738392599\n",
            "Epoch [1/4], Step [364/391], Loss: 338.4247\n",
            "516.3032531222896\n",
            "Epoch [1/4], Step [365/391], Loss: 494.1204\n",
            "542.118415778404\n",
            "Epoch [1/4], Step [366/391], Loss: 494.7992\n",
            "569.2243365673243\n",
            "Epoch [1/4], Step [367/391], Loss: 503.1255\n",
            "597.6855533956905\n",
            "Epoch [1/4], Step [368/391], Loss: 462.5978\n",
            "627.569831065475\n",
            "Epoch [1/4], Step [369/391], Loss: 533.8825\n",
            "658.9483226187488\n",
            "Epoch [1/4], Step [370/391], Loss: 548.3073\n",
            "691.8957387496862\n",
            "Epoch [1/4], Step [371/391], Loss: 684.3049\n",
            "726.4905256871705\n",
            "Epoch [1/4], Step [372/391], Loss: 751.2037\n",
            "762.815051971529\n",
            "Epoch [1/4], Step [373/391], Loss: 826.7068\n",
            "800.9558045701054\n",
            "Epoch [1/4], Step [374/391], Loss: 557.4919\n",
            "841.0035947986107\n",
            "Epoch [1/4], Step [375/391], Loss: 549.8929\n",
            "883.0537745385413\n",
            "Epoch [1/4], Step [376/391], Loss: 648.5870\n",
            "927.2064632654684\n",
            "Epoch [1/4], Step [377/391], Loss: 770.3676\n",
            "973.5667864287419\n",
            "Epoch [1/4], Step [378/391], Loss: 998.4493\n",
            "1022.2451257501791\n",
            "Epoch [1/4], Step [379/391], Loss: 550.4263\n",
            "1073.357382037688\n",
            "Epoch [1/4], Step [380/391], Loss: 803.2141\n",
            "1127.0252511395724\n",
            "Epoch [1/4], Step [381/391], Loss: 783.9196\n",
            "1183.3765136965512\n",
            "Epoch [1/4], Step [382/391], Loss: 913.1731\n",
            "1242.5453393813789\n",
            "Epoch [1/4], Step [383/391], Loss: 1127.4722\n",
            "1304.672606350448\n",
            "Epoch [1/4], Step [384/391], Loss: 1143.6976\n",
            "1369.9062366679705\n",
            "Epoch [1/4], Step [385/391], Loss: 1017.2631\n",
            "1438.401548501369\n",
            "Epoch [1/4], Step [386/391], Loss: 1249.8788\n",
            "1510.3216259264375\n",
            "Epoch [1/4], Step [387/391], Loss: 1621.5614\n",
            "1585.8377072227595\n",
            "Epoch [1/4], Step [388/391], Loss: 1229.4612\n",
            "1665.1295925838976\n",
            "Epoch [1/4], Step [389/391], Loss: 1429.0277\n",
            "1748.3860722130926\n",
            "Epoch [1/4], Step [390/391], Loss: 1630.2327\n",
            "1835.8053758237472\n",
            "Epoch [1/4], Step [391/391], Loss: 1727.3978\n",
            "1927.5956446149346\n",
            "Epoch [2/4], Step [1/391], Loss: 1413.1445\n",
            "2023.9754268456813\n",
            "Epoch [2/4], Step [2/391], Loss: 1734.4718\n",
            "2125.1741981879654\n",
            "Epoch [2/4], Step [3/391], Loss: 2345.9817\n",
            "2231.432908097364\n",
            "Epoch [2/4], Step [4/391], Loss: 2654.8120\n",
            "2343.0045535022323\n",
            "Epoch [2/4], Step [5/391], Loss: 1674.6735\n",
            "2460.154781177344\n",
            "Epoch [2/4], Step [6/391], Loss: 2109.9910\n",
            "2583.162520236211\n",
            "Epoch [2/4], Step [7/391], Loss: 3019.0933\n",
            "2712.3206462480216\n",
            "Epoch [2/4], Step [8/391], Loss: 2345.4956\n",
            "2847.936678560423\n",
            "Epoch [2/4], Step [9/391], Loss: 2191.7664\n",
            "2990.3335124884443\n",
            "Epoch [2/4], Step [10/391], Loss: 3141.4746\n",
            "3139.850188112867\n",
            "Epoch [2/4], Step [11/391], Loss: 2169.8132\n",
            "3296.8426975185102\n",
            "Epoch [2/4], Step [12/391], Loss: 3938.0732\n",
            "3461.684832394436\n",
            "Epoch [2/4], Step [13/391], Loss: 3798.2791\n",
            "3634.769074014158\n",
            "Epoch [2/4], Step [14/391], Loss: 1812.9403\n",
            "3816.507527714866\n",
            "Epoch [2/4], Step [15/391], Loss: 5271.1382\n",
            "4007.332904100609\n",
            "Epoch [2/4], Step [16/391], Loss: 10387.2939\n",
            "4207.699549305639\n",
            "Epoch [2/4], Step [17/391], Loss: 14295.7051\n",
            "4418.084526770921\n",
            "Epoch [2/4], Step [18/391], Loss: 35596.0781\n",
            "4638.988753109467\n",
            "Epoch [2/4], Step [19/391], Loss: 49159.0234\n",
            "4870.938190764941\n",
            "Epoch [2/4], Step [20/391], Loss: 151523.1719\n",
            "5114.485100303188\n",
            "Epoch [2/4], Step [21/391], Loss: 296905.1250\n",
            "5370.209355318348\n",
            "Epoch [2/4], Step [22/391], Loss: 1144717.6250\n",
            "5638.719823084265\n",
            "Epoch [2/4], Step [23/391], Loss: 2607639.2500\n",
            "5920.655814238478\n",
            "Epoch [2/4], Step [24/391], Loss: 11965653.0000\n",
            "6216.688604950403\n",
            "Epoch [2/4], Step [25/391], Loss: 30560748.0000\n",
            "6527.5230351979235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f8a7c4d2185a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Training loop called here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-f8a7c4d2185a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupdate_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#             if (i+1) % 2 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mtrain_loss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlr_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud5bUO3XHZw5",
        "colab_type": "code",
        "outputId": "ba2a17e6-5a51-4daa-b2a0-954a4f579868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(train_loss_array))\n",
        "print(lr_array)\n",
        "np.savetxt(\"Training Loss vs Learning Rate_1.1\", train_loss_array, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "270\n",
            "[1e-05, 1.1000000000000001e-05, 1.2100000000000003e-05, 1.3310000000000005e-05, 1.4641000000000006e-05, 1.610510000000001e-05, 1.771561000000001e-05, 1.9487171000000013e-05, 2.1435888100000015e-05, 2.357947691000002e-05, 2.5937424601000023e-05, 2.8531167061100026e-05, 3.138428376721003e-05, 3.452271214393104e-05, 3.7974983358324144e-05, 4.177248169415656e-05, 4.594972986357222e-05, 5.0544702849929444e-05, 5.5599173134922395e-05, 6.115909044841464e-05, 6.727499949325611e-05, 7.400249944258173e-05, 8.140274938683991e-05, 8.954302432552392e-05, 9.849732675807631e-05, 0.00010834705943388396, 0.00011918176537727237, 0.0001310999419149996, 0.0001442099361064996, 0.00015863092971714956, 0.00017449402268886454, 0.000191943424957751, 0.00021113776745352613, 0.00023225154419887875, 0.00025547669861876666, 0.00028102436848064335, 0.0003091268053287077, 0.0003400394858615785, 0.00037404343444773637, 0.00041144777789251, 0.00045259255568176106, 0.0004978518112499372, 0.000547636992374931, 0.0006024006916124242, 0.0006626407607736668, 0.0007289048368510335, 0.0008017953205361369, 0.0008819748525897506, 0.0009701723378487258, 0.0010671895716335984, 0.0011739085287969582, 0.0012912993816766541, 0.0014204293198443196, 0.0015624722518287517, 0.001718719477011627, 0.00189059142471279, 0.002079650567184069, 0.002287615623902476, 0.0025163771862927236, 0.0027680149049219963, 0.0030448163954141963, 0.003349298034955616, 0.0036842278384511783, 0.004052650622296297, 0.004457915684525927, 0.00490370725297852, 0.005394077978276373, 0.0059334857761040105, 0.006526834353714412, 0.007179517789085854, 0.00789746956799444, 0.008687216524793885, 0.009555938177273274, 0.010511531995000602, 0.011562685194500663, 0.01271895371395073, 0.013990849085345805, 0.015389933993880386, 0.016928927393268425, 0.01862182013259527, 0.020484002145854798, 0.02253240236044028, 0.024785642596484306, 0.02726420685613274, 0.029990627541746015, 0.032989690295920616, 0.03628865932551268, 0.039917525258063954, 0.043909277783870354, 0.04830020556225739, 0.053130226118483136, 0.05844324873033145, 0.0642875736033646, 0.07071633096370106, 0.07778796406007117, 0.08556676046607829, 0.09412343651268613, 0.10353578016395476, 0.11388935818035024, 0.12527829399838528, 0.13780612339822382, 0.1515867357380462, 0.16674540931185083, 0.18341995024303592, 0.20176194526733954, 0.2219381397940735, 0.24413195377348088, 0.268545149150829, 0.29539966406591195, 0.3249396304725032, 0.3574335935197535, 0.3931769528717289, 0.4324946481589018, 0.475744112974792, 0.5233185242722712, 0.5756503766994984, 0.6332154143694483, 0.6965369558063932, 0.7661906513870326, 0.8428097165257359, 0.9270906881783095, 1.0197997569961406, 1.1217797326957548, 1.2339577059653304, 1.3573534765618636, 1.49308882421805, 1.6423977066398552, 1.806637477303841, 1.9873012250342252, 2.186031347537648, 2.4046344822914127, 2.645097930520554, 2.9096077235726097, 3.200568495929871, 3.520625345522858, 3.8726878800751443, 4.259956668082659, 4.685952334890926, 5.154547568380019, 5.670002325218021, 6.237002557739824, 6.860702813513806, 7.546773094865188, 8.301450404351707, 9.131595444786878, 10.044754989265567, 11.049230488192125, 12.154153537011338, 13.369568890712472, 14.70652577978372, 16.17717835776209, 17.794896193538303, 19.574385812892135, 21.53182439418135, 23.685006833599488, 26.05350751695944, 28.658858268655386, 31.524744095520926, 34.67721850507302, 38.144940355580324, 41.95943439113836, 46.1553778302522, 50.770915613277424, 55.84800717460517, 61.432807892065696, 67.57608868127227, 74.3336975493995, 81.76706730433946, 89.94377403477341, 98.93815143825076, 108.83196658207584, 119.71516324028343, 131.6866795643118, 144.85534752074298, 159.34088227281728, 175.27497050009902, 192.80246755010893, 212.08271430511985, 233.29098573563186, 256.6200843091951, 282.2820927401146, 310.51030201412607, 341.5613322155387, 375.7174654370926, 413.2892119808019, 454.61813317888215, 500.0799464967704, 550.0879411464475, 605.0967352610924, 665.6064087872016, 732.1670496659219, 805.3837546325142, 885.9221300957656, 974.5143431053423, 1071.9657774158766, 1179.1623551574644, 1297.078590673211, 1426.7864497405321, 1569.4650947145856, 1726.4116041860443, 1899.052764604649, 2088.958041065114, 2297.8538451716254, 2527.639229688788, 2780.403152657667, 3058.4434679234337, 3364.287814715777, 3700.716596187355, 4070.788255806091, 4477.8670813867, 4925.6537895253705, 5418.219168477908, 5960.0410853257, 6556.04519385827, 7211.649713244097, 7932.814684568508, 8726.096153025359, 9598.705768327896, 10558.576345160687, 11614.433979676756, 12775.877377644432, 14053.465115408877, 15458.811626949766, 17004.692789644745, 18705.16206860922, 20575.678275470145, 22633.24610301716, 24896.57071331888, 27386.22778465077, 30124.85056311585, 33137.33561942744, 36451.069181370185, 40096.1760995072, 44105.793709457925, 48516.37308040372, 53368.0103884441, 58704.811427288514, 64575.29257001737, 71032.82182701911, 78136.10400972104, 85949.71441069315, 94544.68585176248, 103999.15443693874, 114399.06988063263, 125838.9768686959, 138422.8745555655, 152265.16201112207, 167491.67821223428, 184240.84603345772, 202664.9306368035, 222931.42370048387, 245224.56607053228, 269747.02267758554, 296721.72494534415, 326393.8974398786, 359033.2871838665, 394936.6159022532, 434430.27749247855, 477873.30524172646, 525660.6357658991, 578226.699342489, 636049.3692767379, 699654.3062044118, 769619.736824853, 846581.7105073384, 931239.8815580723, 1024363.8697138797, 1126800.2566852677, 1239480.2823537944, 1363428.310589174]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD_qEKL4dNfe",
        "colab_type": "code",
        "outputId": "5b783cc4-f584-4b2b-e6e0-0c0d3267cd1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y = train_loss_array[0:270]\n",
        "x = lr_array[0:270] \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(2, 1, 1)\n",
        "ax.set_xscale('log')\n",
        "plt.plot(x,y)\n",
        "# plt.plot(np.squeeze(train_loss_array[0:150]))\n",
        "plt.ylabel('Training_Loss')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.title(\"Training Loss vs Learning Rate_(eta_t+1 = 1.05eta_t)\")\n",
        "plt.savefig('Training Loss vs Learning Rate_(eta_t+1 = 1.05eta_t).jpg')\n",
        "plt.show()\n",
        "# np.savetxt(\"Training Loss vs Learning Rate_1.1\", train_loss_array, delimiter=\",\")\n",
        "\n",
        "#local minimum 0.3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAACkCAYAAACEsRmlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDYmwuLuc5sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}